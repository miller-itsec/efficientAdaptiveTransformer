\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\catcode 95\active
\bibstyle{plainnat}
\citation{devlin2019bert,vaswani2017attention}
\citation{hinton2015distill,sanh2019distilbert,turc2019wellread}
\citation{goyal2020powerbert,kim2021learned}
\citation{zaheer2020bigbird,beltagy2020longformer,wang2020linformer,choromanski2021performer,kitaev2020reformer}
\citation{xin2020deebert,zhou2020bert}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contributions.}{1}{section*.1}\protected@file@percent }
\citation{vaswani2017attention}
\citation{devlin2019bert}
\citation{hinton2015distill}
\citation{sanh2019distilbert}
\citation{turc2019wellread}
\citation{goyal2020powerbert}
\citation{kim2021learned}
\citation{zaheer2020bigbird}
\citation{beltagy2020longformer}
\citation{wang2020linformer}
\citation{choromanski2021performer}
\citation{kitaev2020reformer}
\citation{xin2020deebert}
\citation{zhou2020bert}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Layer-wise token pruning (at layer $\ell $)}}{2}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:prune}{{1}{2}{Layer-wise token pruning (at layer $\ell $)}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transformer encoder.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model compression.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Token pruning.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sparse attention.}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Early exits.}{2}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method: Efficient Adaptive Transformer}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Progressive token pruning}{2}{subsection.3.1}\protected@file@percent }
\citation{zhou2020bert}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {EAT}{} overview: prune low-importance tokens across depth; use sparse attention per layer; early exit if prediction is confident/stable.}}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:architecture}{{1}{3}{\eat {} overview: prune low-importance tokens across depth; use sparse attention per layer; early exit if prediction is confident/stable}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sparse attention}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Early exits and confidence}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Training Objectives and Schedule}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Schedule.}{3}{section*.8}\protected@file@percent }
\citation{devlin2019bert}
\citation{sanh2019distilbert}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Two-stage fine-tuning with annealed pruning}}{4}{algorithm.2}\protected@file@percent }
\newlabel{alg:train}{{2}{4}{Two-stage fine-tuning with annealed pruning}{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Theoretical Considerations}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implication.}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimental Setup}{4}{section.6}\protected@file@percent }
\newlabel{sec:exp}{{6}{4}{Experimental Setup}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Token pruning progression in \textsc  {EAT}{}: scheduled vs.\ observed retention.}}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pruning}{{2}{5}{Token pruning progression in \eat {}: scheduled vs.\ observed retention}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Ablation on \textbf  {SST-2}: normalized compute and accuracy deltas across variants, generated directly from logged results.}}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:flops}{{3}{5}{Ablation on \textbf {SST-2}: normalized compute and accuracy deltas across variants, generated directly from logged results}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results and Discussion}{5}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Accuracy--latency frontiers generated by \texttt  {plot\_frontiers.py}. \textsc  {EAT}{} (orange, labeled by $\tau $) bridges \textsc  {BERT}{} (black) and \textsc  {DistilBERT}{} (blue), enabling task-specific operating points between static and dynamic inference.}}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:frontiers}{{4}{6}{Accuracy--latency frontiers generated by \texttt {plot\_frontiers.py}. \eat {} (orange, labeled by $\tau $) bridges \bert {} (black) and \distilbert {} (blue), enabling task-specific operating points between static and dynamic inference}{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Frontier metrics on \textbf  {SST-2}, automatically generated from the logged CSV.}}{6}{table.caption.13}\protected@file@percent }
\newlabel{tab:summary-sst2}{{1}{6}{Frontier metrics on \textbf {SST-2}, automatically generated from the logged CSV}{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Overall Frontiers}{6}{subsection.7.1}\protected@file@percent }
\newlabel{sec:frontiers}{{7.1}{6}{Overall Frontiers}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Observations.}{6}{section*.16}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Frontier metrics on \textbf  {QQP}, automatically generated from the logged CSV.}}{7}{table.caption.14}\protected@file@percent }
\newlabel{tab:summary-qqp}{{2}{7}{Frontier metrics on \textbf {QQP}, automatically generated from the logged CSV}{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Frontier metrics on \textbf  {MNLI (matched)}, automatically generated from the logged CSV.}}{7}{table.caption.15}\protected@file@percent }
\newlabel{tab:summary-mnli}{{3}{7}{Frontier metrics on \textbf {MNLI (matched)}, automatically generated from the logged CSV}{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Calibration and Reliability}{7}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Analysis of Adaptive Behavior}{7}{subsection.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Final token retention as a function of input sequence length.}}{8}{figure.caption.17}\protected@file@percent }
\newlabel{fig:retention-plot}{{5}{8}{Final token retention as a function of input sequence length}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of model exit layers for the MNLI task at $\tau =0.90$.}}{8}{figure.caption.17}\protected@file@percent }
\newlabel{fig:exit-plot}{{6}{8}{Distribution of model exit layers for the MNLI task at $\tau =0.90$}{figure.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation on \textbf  {SST-2}, generated automatically from logged results.}}{8}{table.caption.18}\protected@file@percent }
\newlabel{tab:ablation}{{4}{8}{Ablation on \textbf {SST-2}, generated automatically from logged results}{table.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Security Applications}{8}{subsection.7.4}\protected@file@percent }
\newlabel{sec:securityapps}{{7.4}{8}{Security Applications}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Ablation Study}{8}{subsection.7.5}\protected@file@percent }
\newlabel{sec:ablation}{{7.5}{8}{Ablation Study}{subsection.7.5}{}}
\citation{teerapittayanon2016branchynet}
\citation{xin2020deebert}
\citation{liu2020fastbert}
\citation{chen2021elasticbert}
\citation{fan2019layerdrop}
\citation{jiao2020tinybert}
\citation{gordon2020compressing}
\citation{beltagy2020longformer}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{9}{section.8}\protected@file@percent }
\newlabel{sec:conclusion}{{8}{9}{Conclusion}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Related Work}{9}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Adaptive Transformers and Early Exiting}{9}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Pruning and Sparse Attention}{9}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Efficiency Evaluation and Reproducibility}{9}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Deployment and Threat Model Considerations}{9}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Limitations and Future Work}{9}{section.11}\protected@file@percent }
\newlabel{sec:limitations}{{11}{9}{Limitations and Future Work}{section.11}{}}
\citation{hou2020dynabert}
\bibdata{references}
\bibcite{beltagy2020longformer}{{1}{2020}{{Beltagy et~al.}}{{Beltagy, Peters, and Cohan}}}
\bibcite{chen2021elasticbert}{{2}{2021}{{Chen et~al.}}{{Chen, Lou, Huang, et~al.}}}
\bibcite{choromanski2021performer}{{3}{2021}{{Choromanski et~al.}}{{Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Weller, and Kuna}}}
\bibcite{devlin2019bert}{{4}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Conclusion}{10}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Checklist and Scripts}{10}{appendix.A}\protected@file@percent }
\bibcite{fan2019layerdrop}{{5}{2019}{{Fan et~al.}}{{Fan, Grave, and Joulin}}}
\bibcite{gordon2020compressing}{{6}{2020}{{Gordon et~al.}}{{Gordon, Duh, and Andrews}}}
\bibcite{goyal2020powerbert}{{7}{2020}{{Goyal et~al.}}{{Goyal, Kapoor, Nenkova, and Neubig}}}
\bibcite{hinton2015distill}{{8}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{hou2020dynabert}{{9}{2020}{{Hou et~al.}}{{Hou, Huang, Shang, Jiang, Chen, and Liu}}}
\bibcite{jiao2020tinybert}{{10}{2020}{{Jiao et~al.}}{{Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu}}}
\bibcite{kim2021learned}{{11}{2021}{{Kim et~al.}}{{Kim, Lee, and Kim}}}
\bibcite{kitaev2020reformer}{{12}{2020}{{Kitaev et~al.}}{{Kitaev, Kaiser, and Levskaya}}}
\bibcite{liu2020fastbert}{{13}{2020}{{Liu et~al.}}{{Liu, Zhou, Zhao, Wang, Ju, Deng, and Wang}}}
\bibcite{sanh2019distilbert}{{14}{2019}{{Sanh et~al.}}{{Sanh, Debut, Chaumond, and Wolf}}}
\bibcite{teerapittayanon2016branchynet}{{15}{2016}{{Teerapittayanon et~al.}}{{Teerapittayanon, McDanel, and Kung}}}
\bibcite{turc2019wellread}{{16}{2019}{{Turc et~al.}}{{Turc, Chang, Lee, and Toutanova}}}
\bibcite{vaswani2017attention}{{17}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2020linformer}{{18}{2020}{{Wang et~al.}}{{Wang, Li, Khabsa, Fang, and Ma}}}
\bibcite{xin2020deebert}{{19}{2020}{{Xin et~al.}}{{Xin, Tang, Yu, Yu, and Lin}}}
\bibcite{zaheer2020bigbird}{{20}{2020}{{Zaheer et~al.}}{{Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Ontanon, Pham, Ravula, Wang, Yang, and Ahmed}}}
\bibcite{zhou2020bert}{{21}{2020}{{Zhou et~al.}}{{Zhou, Sun, Li, Zhang, and Lin}}}
\gdef \@abspage@last{12}

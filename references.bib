@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019},
  url={https://aclanthology.org/N19-1423}
}

@article{hinton2015distill,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv:1503.02531},
  year={2015}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv:1910.01108},
  year={2019}
}

@article{turc2019wellread,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv:1908.08962},
  year={2019}
}

@inproceedings{zaheer2020bigbird,
  title={Big Bird: Transformers for Longer Sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv:2004.05150},
  year={2020}
}

@inproceedings{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{choromanski2021performer,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Belanger, David and Weller, Adrian and Kuna, Michael},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{hou2020dynabert,
  title={DynaBERT: Dynamic BERT with Adaptive Width and Depth},
  author={Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{xin2020deebert,
  title={DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  author={Xin, Ji and Tang, Raphael and Yu, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  booktitle={ACL},
  year={2020},
  url={https://aclanthology.org/2020.acl-main.204}
}

@article{zhou2020bert,
  title={BERT Loses Patience: Fast and Robust Inference with Early Exit},
  author={Zhou, Wangchunshu and Sun, Qizhe and Li, Xuezhi and Zhang, Yang and Lin, Zhouhan},
  journal={arXiv:2006.04152},
  year={2020}
}

@inproceedings{goyal2020powerbert,
  title={{PoWER-BERT}: Accelerating {BERT} Inference via Progressive Word-vector Elimination},
  author={Goyal, Sandeep Subramanian and Kapoor, Vikrant and Nenkova, Ani and Neubig, Graham},
  booktitle={ICML Workshop on Efficient Natural Language and Speech Processing},
  year={2020},
  note={Archival arXiv version: arXiv:2001.08950}
}

@article{kim2021learned,
  title={Learned Token Pruning for Transformers},
  author={Kim, Tae-Hyoung and Lee, Hyeonseob and Kim, Sungroh},
  journal={arXiv:2107.00910},
  year={2021}
}

@inproceedings{teerapittayanon2016branchynet,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Teerapittayanon, Surat and McDanel, Bradley and Kung, Hsiang-Tsung},
  booktitle={Proc. ICPR},
  year={2016}
}

@inproceedings{fan2019layerdrop,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={Proc. ICLR},
  year={2019}
}

@article{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={EMNLP},
  year={2020}
}

@article{gordon2020compressing,
  title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
  author={Gordon, Mitchell and Duh, Kevin and Andrews, Nicholas},
  journal={ACL},
  year={2020}
}

@article{liu2020fastbert,
  title={FastBERT: a Self-distilling BERT with Adaptive Inference Time},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
  journal={ACL},
  year={2020}
}

@article{chen2021elasticbert,
  title={ElasticBERT: Neural Architecture Search for Compact and Fast BERT},
  author={Chen, Guimin and Lou, Jun and Huang, Xun and others},
  journal={NeurIPS},
  year={2021}
}
